{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlk888/Yandex_ML-contest3_25s/blob/main/homeworks/hw01_classification/02_hw_fmnist_classification_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsVMGiVgSq2"
      },
      "source": [
        "## Классификация FashionMNIST\n",
        "\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/girafe_ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3isBRG6PgSq6"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jUZ7kCOkH9ad"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "\n",
        "def parse_pytorch_model(model_str):\n",
        "    def parse_layer(layer_str):\n",
        "        layer_info = {}\n",
        "        layer_name, params = layer_str.split(\"(\", 1)\n",
        "        params = params.rstrip(\")\")\n",
        "        layer_info[\"type\"] = layer_name.strip()\n",
        "        param_dict = {}\n",
        "        for param in params.split(\", \"):\n",
        "            if \"=\" in param:\n",
        "                key, value = param.split(\"=\")\n",
        "                param_dict[key.strip()] = eval(value.strip())\n",
        "            else:\n",
        "                param_dict[param.strip()] = None\n",
        "        layer_info[\"parameters\"] = param_dict\n",
        "        return layer_info\n",
        "\n",
        "    model_dict = {}\n",
        "    lines = model_str.splitlines()\n",
        "    model_name = lines[0].strip(\"()\")\n",
        "    model_dict[\"model_name\"] = model_name\n",
        "    model_dict[\"layers\"] = []\n",
        "\n",
        "    layer_regex = re.compile(r\"\\((\\d+)\\): (.+)\")\n",
        "    for line in lines[1:]:\n",
        "        line = line.strip()\n",
        "        match = layer_regex.match(line)\n",
        "        if match:\n",
        "            index, layer = match.groups()\n",
        "            model_dict[\"layers\"].append({\"index\": int(index), \"layer\": parse_layer(layer)})\n",
        "    return model_dict\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eMTRpZgdH9ad"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "def get_predictions(model, eval_data, step=10):\n",
        "\n",
        "    predicted_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, len(eval_data), step):\n",
        "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    predicted_labels = []\n",
        "    real_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            y_predicted = model(batch[0].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "            real_labels.append(batch[1])\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    real_labels = torch.cat(real_labels)\n",
        "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
        "    return accuracy_score\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDrV0BJIH9ae"
      },
      "source": [
        "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p7f8d7AAH9ae",
        "outputId": "c960d545-1d9c-4715-e436-a9f54b19880a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-09 14:33:10--  https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict [following]\n",
            "--2025-04-09 14:33:11--  https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6272446 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘hw_overfitting_data_dict.npy’\n",
            "\n",
            "hw_overfitting_data 100%[===================>]   5.98M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-04-09 14:33:11 (65.0 MB/s) - ‘hw_overfitting_data_dict.npy’ saved [6272446/6272446]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict -O hw_overfitting_data_dict.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zQWqvLoxH9ae"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeA6Q5-CgSq7"
      },
      "source": [
        "Вернемся к задаче распознавания простых изображений, рассмотренной ранее. Но теперь будем работать с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). В данном задании воспользуемся всем датасетом целиком.\n",
        "\n",
        "__Ваша первая задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n",
        "\n",
        "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбукам с предыдущих занятий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pwyJN-B-H9af"
      },
      "outputs": [],
      "source": [
        "CUDA_DEVICE_ID = 0  # change if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nPG1KbQAgl8b"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = (\n",
        "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "aYcL28OsgSq8",
        "outputId": "92788fed-9541-4955-81cc-43cf728d8d96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Image label: 0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "train_fmnist_data = FashionMNIST(\n",
        "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "test_fmnist_data = FashionMNIST(\n",
        "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "random_batch = next(iter(train_data_loader))\n",
        "_image, _label = random_batch[0][0], random_batch[1][0]\n",
        "plt.figure()\n",
        "plt.imshow(_image.reshape(28, 28))\n",
        "plt.title(f\"Image label: {_label}\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6jWRv1rgSq8"
      },
      "source": [
        "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
        "\n",
        "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BcyEFX-RgSq8"
      },
      "outputs": [],
      "source": [
        "# Creating model instance\n",
        "model_task_1 = None\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        # Channel attention\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # Spatial attention\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel\n",
        "        b, c, _, _ = x.size()\n",
        "        avg = self.avg_pool(x).view(b, c)\n",
        "        max = self.max_pool(x).view(b, c)\n",
        "        channel_weights = self.fc(avg + max).view(b, c, 1, 1)\n",
        "        x = x * channel_weights\n",
        "        # Spatial\n",
        "        avg = torch.mean(x, dim=1, keepdim=True)\n",
        "        max, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        spatial = torch.cat([avg, max], dim=1)\n",
        "        spatial_weights = torch.sigmoid(self.conv(spatial))\n",
        "        return x * spatial_weights\n",
        "\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            ResidualBlock(1, 32),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            ResidualBlock(32, 64),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Block 3\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock(64, 128),\n",
        "            CBAM(128)\n",
        "        )\n",
        "        # Block 4\n",
        "        self.block4 = nn.Sequential(\n",
        "            ResidualBlock(128, 256),\n",
        "            CBAM(256)\n",
        "        )\n",
        "        # Classifier\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.gap(x).squeeze()\n",
        "        return self.fc(x)\n",
        "\n",
        "# Создание экземпляра модели\n",
        "model_task_1 = FashionMNISTModel()\n",
        "#for m in model_task_1.modules():\n",
        "#    if isinstance(m, nn.Conv2d):\n",
        "#        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAoLV4dkoy5M"
      },
      "source": [
        "Не забудьте перенести модель на выбранный `device`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Xas9SIXDoxvZ",
        "outputId": "15e0a548-4391-4f64-e693-c09ccc3324bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModel(\n",
              "  (block1): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block2): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block3): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): CBAM(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=8, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=8, out_features=128, bias=True)\n",
              "        (3): Sigmoid()\n",
              "      )\n",
              "      (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    )\n",
              "  )\n",
              "  (block4): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): CBAM(\n",
              "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
              "      (fc): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=16, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=16, out_features=256, bias=True)\n",
              "        (3): Sigmoid()\n",
              "      )\n",
              "      (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    )\n",
              "  )\n",
              "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (1): Dropout(p=0.3, inplace=False)\n",
              "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model_task_1.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pLRWysggSq9"
      },
      "source": [
        "Локальные тесты для проверки вашей модели доступны ниже:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qMQzo1ggSq9",
        "outputId": "937fbdf1-8e1d-4678-cecb-f21a5ff12193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
        "\n",
        "try:\n",
        "    x = random_batch[0].to(device)\n",
        "    y = random_batch[1].to(device)\n",
        "\n",
        "    # compute outputs given inputs, both are variables\n",
        "    y_predicted = model_task_1(x)\n",
        "except Exception as e:\n",
        "    print(\"Something is wrong with the model\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
        "\n",
        "print(\"Everything seems fine!\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRmIPwIgSq9"
      },
      "source": [
        "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YJnU14bdnZa_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "0ea4cc68-875b-404a-fcb9-95639df03e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-025341ca488e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Обратный проход и шаг оптимизации\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Оптимизатор: AdamW с начальным LR=0.0005 и weight_decay=1e-4 для L2-регуляризации\n",
        "optimizer = torch.optim.Adam(model_task_1.parameters(), lr=0.0005)\n",
        "\n",
        "# Scheduler: ReduceLROnPlateau для динамического изменения LR\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "# Функция потерь: CrossEntropyLoss для задачи классификации\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Количество эпох\n",
        "num_epochs = 30\n",
        "\n",
        "# Переменные для ранней остановки\n",
        "best_test_acc = 0.0\n",
        "patience_counter = 0\n",
        "patience_limit = 5  # Остановка после 5 эпох без улучшений\n",
        "\n",
        "# Цикл обучения\n",
        "for epoch in range(num_epochs):\n",
        "    # Режим обучения\n",
        "    model_task_1.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Обнуляем градиенты\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Прямой проход\n",
        "        outputs = model_task_1(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Обратный проход и шаг оптимизации\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Суммируем Train Loss\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Средний Train Loss за эпоху\n",
        "    train_loss /= len(train_data_loader.dataset)\n",
        "\n",
        "    # Оценка на тестовом наборе\n",
        "    model_task_1.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model_task_1(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Test Accuracy\n",
        "    test_acc = 100 * correct / total\n",
        "\n",
        "    # Обновляем scheduler на основе Test Accuracy\n",
        "    scheduler.step(test_acc)\n",
        "\n",
        "    # Текущий LR\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Вывод метрик\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.2f}%, LR: {current_lr:.6f}\")\n",
        "\n",
        "    # Сохранение лучшей модели и ранняя остановка\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        torch.save(model_task_1.state_dict(), 'best_model.pth')\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience_limit:\n",
        "            print(f\"Ранняя остановка на эпохе {epoch+1}. Лучшая Test Accuracy: {best_test_acc:.2f}%\")\n",
        "            break\n",
        "\n",
        "if patience_counter < patience_limit:\n",
        "    print(f\"Обучение завершено. Лучшая Test Accuracy: {best_test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zce7gt1gSq-"
      },
      "source": [
        "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usswrWYOgSq-"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Xua3TVZHgSq-",
        "outputId": "5e33192e-115c-459b-8e09-db0f9bba53f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on train set: 0.99992\n"
          ]
        }
      ],
      "source": [
        "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "l9KEKXBxgSq-",
        "outputId": "c0351949-396c-4826-ca37-39e6a88204a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on test set: 0.9303\n"
          ]
        }
      ],
      "source": [
        "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oyhmMobgSq_"
      },
      "source": [
        "Проверка, что необходимые пороги пройдены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAIrURCEgSq_",
        "outputId": "ecaec63d-c4bd-44fa-e556-6e95a510c022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Test Accuracy: 93.03%\n",
            "\n",
            "Class-wise Accuracy:\n",
            "T-shirt/top    : 89.40%\n",
            "Trouser        : 98.60%\n",
            "Pullover       : 90.80%\n",
            "Dress          : 93.30%\n",
            "Coat           : 90.60%\n",
            "Sandal         : 98.60%\n",
            "Shirt          : 75.80%\n",
            "Sneaker        : 97.90%\n",
            "Bag            : 98.10%\n",
            "Ankle boot     : 97.20%\n"
          ]
        }
      ],
      "source": [
        "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
        "assert (\n",
        "    train_acc_task_1 >= 0.905\n",
        "), \"Train accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\"\n",
        "\n",
        "def class_wise_accuracy(model, test_loader, num_classes=10):\n",
        "    model.eval()\n",
        "    correct_pred = {class_idx: 0 for class_idx in range(num_classes)}\n",
        "    total_pred = {class_idx: 0 for class_idx in range(num_classes)}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "            # Update counts for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                class_idx = label.item()\n",
        "                total_pred[class_idx] += 1\n",
        "                if prediction == label:\n",
        "                    correct_pred[class_idx] += 1\n",
        "\n",
        "    # Calculate accuracy for each class\n",
        "    class_acc = {}\n",
        "    for class_idx in range(num_classes):\n",
        "        if total_pred[class_idx] > 0:\n",
        "            acc = 100 * correct_pred[class_idx] / total_pred[class_idx]\n",
        "            class_acc[class_idx] = acc\n",
        "        else:\n",
        "            class_acc[class_idx] = 0.0  # Handle case with zero samples\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    total_correct = sum(correct_pred.values())\n",
        "    total_samples = sum(total_pred.values())\n",
        "    overall_acc = 100 * total_correct / total_samples\n",
        "\n",
        "    return class_acc, overall_acc\n",
        "\n",
        "class_acc, overall_acc = class_wise_accuracy(model_task_1, test_data_loader)\n",
        "\n",
        "# Вывод результатов\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print(f\"Overall Test Accuracy: {overall_acc:.2f}%\")\n",
        "print(\"\\nClass-wise Accuracy:\")\n",
        "for idx, name in enumerate(class_names):\n",
        "    print(f\"{name:15}: {class_acc[idx]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVpYzE23H9ah"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`, а файл `hw_fmnist_data_dict.npy` находится в той же директории, что и ноутбук (он доступен в репозитории)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "0sb8aZenH9ah",
        "outputId": "39c63119-e085-42df-9cc7-8af694df136b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_fmnist_task_1.json`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_fmnist_data_dict.npy\"\n",
        "), \"Please, download `hw_fmnist_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_fmnist_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict = {\n",
        "    \"train_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "    ),\n",
        "    \"test_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "    ),\n",
        "    \"model_task_1\": parse_pytorch_model(str(model_task_1)),\n",
        "}\n",
        "\n",
        "with open(\"submission_dict_fmnist_task_1.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_fmnist_task_1.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDthBDWeH9ah"
      },
      "source": [
        "### Сдача задания\n",
        "Сдайте сгенерированный файл в соответствующую задачу в соревновании, а именно:\n",
        "    \n",
        "* `submission_dict_fmnist_task_1.json` в задачу Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtWnYAN_gSrA"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "facelv_1.13+cu117",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "21499ab2a6726e29f7050b76af0e9680227e613293d630ba279de7ebdfad9cae"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}